{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8facf5b",
   "metadata": {},
   "source": [
    "### **- SPARK**\n",
    "\n",
    "- Esse será uma ula de visão geral sobre Spark. Veremos:\n",
    "\n",
    "    - Spark \n",
    "    - Spark vs MapReduce\n",
    "    - Spark RDDs\n",
    "    - Operações RDDs\n",
    "\n",
    "\n",
    "- Não se preocupe caso não entenda todas as operações, veremos novamente <br>\n",
    "  quando programarmos com Python e Spark\n",
    "\n",
    "\n",
    "- Spark é uma das mais novas tecnologias usadas no tratamento de BigData <br>\n",
    "\n",
    "\n",
    "- É um projeto de código fonte aberto no Apache <br>\n",
    "\n",
    "\n",
    "- Seu primeiro lançamento foi feito em fevereiro de 2013 e ganhou muita popularidade <br>\n",
    "  devido a sua facilidade e velocidade \n",
    "\n",
    "\n",
    "- Você deve imaginar o Spark como uma alternativa ao MapReduce <br>\n",
    "\n",
    "\n",
    "- Spark usa dados de uma fonte variável de dados:\n",
    "    \n",
    "     - Cassandra\n",
    "     - AWS S3\n",
    "     - HDFS\n",
    "     - E mais!\n",
    "\n",
    "\n",
    "### **- Spark vs MapReduce**\n",
    "\n",
    "\n",
    "- MapReduce necessita que os dois sejam guardados no HDFS. Spark <br>\n",
    "  Não!\n",
    "\n",
    "\n",
    "- Spark consegue realizar operações até $100x$ mais rápido que MapReduce <br>\n",
    "\n",
    "\n",
    "- Como ele consegue isso ? <br>\n",
    "\n",
    "\n",
    "- MapReduce escreve a maioria dos dados no disco após cada operação. <br>\n",
    "\n",
    "\n",
    "- Spark mantém a maioria dos dados na memória RAM. <br>\n",
    "\n",
    "\n",
    "- Spark apenas usa o HD quando a memória está cheia.\n",
    "\n",
    "\n",
    "### **-RDDs Spark**\n",
    "\n",
    "\n",
    "- No nucléo do Spark está a ideia do RDD - Resilient Distributed Dataset <br>\n",
    "  (ou Conjunto de dados distribuídos resiliente).\n",
    "\n",
    "\n",
    "- RDDs possuem 4 características pricipais: \n",
    "    \n",
    "    - Distribuição de conjunto de dados\n",
    "    - Tolerância à falha\n",
    "    - Operações em paralelo\n",
    "    - Habilidade de usar diversas fontes de dados\n",
    "\n",
    "- Basicamente existem dois tipos de operações com RDDs: \n",
    "    \n",
    "    - Transformações\n",
    "    - Ações\n",
    "\n",
    "- Ações básicas: \n",
    "    \n",
    "    - First\n",
    "    - Collect\n",
    "    - Count\n",
    "    - Take\n",
    "\n",
    "- First: Retorna o primeiro elemento de um RDD. <br>\n",
    "\n",
    "\n",
    "- Collect: Retorna todos os elementos de um RDD como um array no drive do programa. <br>\n",
    "  \n",
    "\n",
    "- Count: Retorna o nucléo o número de elementos de um RDD. <br>\n",
    "\n",
    "\n",
    "- Take: Retorna um array com os primeiros n elementos de um RDD. <br>\n",
    "\n",
    "\n",
    "- RDD.filter(): \n",
    "   \n",
    "   - Aplicado uma função a cada elemento e retorna apenas elementos que <br>\n",
    "      retornaram True.\n",
    "\n",
    "\n",
    "- RDD.map(): \n",
    "   \n",
    "   - Transformando cada elemento e preserva o número de elementos. Similar ao\n",
    "     .apply() do pandas.\n",
    "\n",
    "\n",
    "- RDD.flatMap(): \n",
    "\n",
    "    - Transforma cada elemento em 0-N elementos. Muda o número de elementos.\n",
    "\n",
    "\n",
    "\n",
    "### Map vs flatMap\n",
    "\n",
    "\n",
    "- Map():\n",
    "    \n",
    "    - Obtendo a primeira letra de uma lista de nomes\n",
    "    \n",
    "\n",
    "- FlapMap():\n",
    "    \n",
    "    - Transformando um corpo de texto em uma lista de palavras\n",
    "    \n",
    "\n",
    "- Isso ficará mais claro quando programarmos com PySpark\n",
    "\n",
    "### Reduce vs ReduceByKey\n",
    "\n",
    "\n",
    "- Reduce():\n",
    "    - Uma ação que irá agregar os elementos do RDD usando uma função que\n",
    "      retorna um elemento apenas.\n",
    "\n",
    "\n",
    "- ReduceByKey() \n",
    "\n",
    "    - Uma ação que irá agregar pares de elementos RDDs usando uma função\n",
    "      que retorna um par RDD.\n",
    "\n",
    "\n",
    "- Essa ideia são similares a operação Groupby.\n",
    "\n",
    "\n",
    "### Big Data\n",
    "\n",
    "\n",
    "- Spark está em constante desenvolvimento e novas atualizações saem a <br>\n",
    "  todos instantes.\n",
    "\n",
    "  \n",
    "- O ecossistema do Spark agora inclui:\n",
    "\n",
    "\n",
    "- Spark SQL:\n",
    "\n",
    "    - Spark DataFrames\n",
    "    - MLlib\n",
    "    - GraphX\n",
    "    - Spark Streaming\n",
    "\n",
    "\n",
    "- Agora aprendemos o suficiente para começar.\n",
    "\n",
    "\n",
    "- Vamos começar criando uma conta no Amazon Web Services para <br>\n",
    "  configurar nosso Spark.\n",
    "  \n",
    "\n",
    "- Teremos tembém um artigo mostrando outras opções, caso você <br>\n",
    "  não queira utilizar AWS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d87329",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468fd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdba3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f9ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215bda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cd1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93c081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdfb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
